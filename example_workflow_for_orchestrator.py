"""
EXEMPLE D'INTÃ‰GRATION POUR L'ORCHESTRATEUR

Ce script montre comment utiliser les 3 agents (Auditeur, Correcteur, Testeur)
dans un workflow complet.

Destinataire : Lead Dev (Orchestrateur)
Auteur : IngÃ©nieur Prompt
Date : 10/01/2026
"""

import os
import json
from dotenv import load_dotenv
import google.generativeai as genai

# Import des fonctions de prompts
from src.prompts import (
    get_auditor_prompt,
    get_fixer_prompt,
    get_judge_prompt,
    PROMPT_VERSIONS,
    ESTIMATED_COSTS,
    print_module_info,
)

# Configuration
load_dotenv()
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
MODEL = genai.GenerativeModel('gemini-2.5-flash')


def run_complete_workflow(file_path: str, max_iterations: int = 3):
    """
    ExÃ©cute le workflow complet : Audit -> Fix -> Test -> (repeat if needed).
    
    Args:
        file_path (str): Chemin vers le fichier Python Ã  analyser et corriger
        max_iterations (int): Nombre maximum d'itÃ©rations (protection boucle infinie)
    
    Returns:
        dict: RÃ©sultats du workflow complet
    """
    
    print("\n" + "ğŸ”„" * 40)
    print("WORKFLOW COMPLET : AUDITEUR â†’ CORRECTEUR â†’ TESTEUR")
    print("ğŸ”„" * 40 + "\n")
    
    # Lecture du fichier original
    with open(file_path, 'r', encoding='utf-8') as f:
        original_code = f.read()
    
    file_name = os.path.basename(file_path)
    current_code = original_code
    iteration = 0
    
    workflow_results = {
        "file": file_name,
        "iterations": [],
        "final_status": None,
        "total_bugs_found": 0,
        "total_bugs_fixed": 0,
    }
    
    # Boucle de feedback
    while iteration < max_iterations:
        iteration += 1
        print(f"\n{'='*80}")
        print(f"ITÃ‰RATION {iteration}/{max_iterations}")
        print(f"{'='*80}\n")
        
        iteration_data = {
            "iteration": iteration,
            "audit": None,
            "fix": None,
            "test": None,
        }
        
        # ============================================================
        # Ã‰TAPE 1 : AUDITEUR
        # ============================================================
        print("ğŸ” Ã‰TAPE 1 : AUDIT DU CODE")
        print("-" * 80)
        
        # GÃ©nÃ¨re le prompt avec la fonction helper
        audit_prompt = get_auditor_prompt(file_name, current_code)
        
        # Appelle Gemini
        audit_response = MODEL.generate_content(audit_prompt)
        audit_json_str = audit_response.text.strip()
        
        # Parse le JSON
        if audit_json_str.startswith("```json"):
            audit_json_str = audit_json_str[7:-3].strip()
        
        try:
            audit_report = json.loads(audit_json_str)
        except json.JSONDecodeError:
            print("âŒ ERREUR : JSON invalide de l'Auditeur")
            workflow_results["final_status"] = "AUDIT_JSON_ERROR"
            break
        
        iteration_data["audit"] = audit_report
        
        bugs_found = audit_report.get("total_issues", 0)
        print(f"âœ… Audit terminÃ© : {bugs_found} problÃ¨me(s) dÃ©tectÃ©(s)")
        
        if bugs_found == 0:
            print("âœ¨ Code propre ! Pas de correction nÃ©cessaire.")
            workflow_results["final_status"] = "CLEAN_CODE"
            workflow_results["iterations"].append(iteration_data)
            break
        
        workflow_results["total_bugs_found"] += bugs_found
        
        # ============================================================
        # Ã‰TAPE 2 : CORRECTEUR
        # ============================================================
        print("\nğŸ”§ Ã‰TAPE 2 : CORRECTION DU CODE")
        print("-" * 80)
        
        # GÃ©nÃ¨re le prompt avec la fonction helper
        fix_prompt = get_fixer_prompt(file_name, current_code, audit_report)
        
        # Appelle Gemini
        fix_response = MODEL.generate_content(fix_prompt)
        fixed_code = fix_response.text.strip()
        
        # Nettoie le code (enlÃ¨ve markdown si prÃ©sent)
        if fixed_code.startswith("```python"):
            fixed_code = fixed_code[9:-3].strip()
        elif fixed_code.startswith("```"):
            fixed_code = fixed_code[3:-3].strip()
        
        iteration_data["fix"] = {
            "original_lines": len(current_code.splitlines()),
            "fixed_lines": len(fixed_code.splitlines()),
        }
        
        # VÃ©rifie la syntaxe
        try:
            compile(fixed_code, file_name, 'exec')
            print("âœ… Code corrigÃ© syntaxiquement valide")
        except SyntaxError as e:
            print(f"âŒ ERREUR : Code corrigÃ© invalide : {e}")
            workflow_results["final_status"] = "SYNTAX_ERROR"
            workflow_results["iterations"].append(iteration_data)
            break
        
        current_code = fixed_code
        workflow_results["total_bugs_fixed"] += bugs_found
        
        # ============================================================
        # Ã‰TAPE 3 : TESTEUR (SimulÃ© ici)
        # ============================================================
        print("\nâš–ï¸  Ã‰TAPE 3 : TESTS (SIMULÃ‰)")
        print("-" * 80)
        
        # NOTE : Dans un vrai systÃ¨me, on exÃ©cuterait pytest ici
        # Pour cet exemple, on simule un succÃ¨s si le code est valide
        
        simulated_pytest_output = f"""
============================= test session starts ==============================
collected 5 items

test_{file_name} .....                                              [100%]

============================== 5 passed in 0.12s ===============================
"""
        
        # GÃ©nÃ¨re le prompt avec la fonction helper
        judge_prompt = get_judge_prompt(file_name, simulated_pytest_output)
        
        # Appelle Gemini
        judge_response = MODEL.generate_content(judge_prompt)
        judge_json_str = judge_response.text.strip()
        
        # Parse le JSON
        if judge_json_str.startswith("```json"):
            judge_json_str = judge_json_str[7:-3].strip()
        
        try:
            judge_report = json.loads(judge_json_str)
        except json.JSONDecodeError:
            print("âŒ ERREUR : JSON invalide du Testeur")
            workflow_results["final_status"] = "JUDGE_JSON_ERROR"
            break
        
        iteration_data["test"] = judge_report
        
        decision = judge_report.get("decision", "UNKNOWN")
        print(f"âœ… DÃ©cision du Testeur : {decision}")
        
        workflow_results["iterations"].append(iteration_data)
        
        if decision == "VALIDATE":
            workflow_results["final_status"] = "VALIDATED"
            print("\nğŸ‰ CODE VALIDÃ‰ ! Workflow terminÃ©.")
            break
        elif decision == "PASS_TO_FIXER":
            print("\nâš ï¸  Tests Ã©chouÃ©s. Nouvelle itÃ©ration nÃ©cessaire...")
            # Dans un vrai systÃ¨me, on rÃ©injecterait les erreurs au Correcteur
            continue
    
    # Timeout
    if iteration >= max_iterations and workflow_results["final_status"] is None:
        workflow_results["final_status"] = "MAX_ITERATIONS_REACHED"
        print(f"\nâš ï¸  Limite de {max_iterations} itÃ©rations atteinte.")
    
    # RÃ©sumÃ© final
    print("\n" + "=" * 80)
    print("ğŸ“Š RÃ‰SUMÃ‰ DU WORKFLOW")
    print("=" * 80)
    print(f"Fichier         : {file_name}")
    print(f"ItÃ©rations      : {len(workflow_results['iterations'])}")
    print(f"Bugs dÃ©tectÃ©s   : {workflow_results['total_bugs_found']}")
    print(f"Bugs corrigÃ©s   : {workflow_results['total_bugs_fixed']}")
    print(f"Status final    : {workflow_results['final_status']}")
    print("=" * 80 + "\n")
    
    return workflow_results


def example_usage():
    """Exemple d'utilisation pour l'Orchestrateur."""
    
    # Affiche les infos du module
    print_module_info()
    
    # Exemple de workflow complet
    print("\n\nğŸ¯ EXEMPLE DE WORKFLOW COMPLET\n")
    
    # Fichier de test
    test_file = "sandbox/test_samples/buggy_code_simple.py"
    
    if not os.path.exists(test_file):
        print(f"âŒ Fichier de test non trouvÃ© : {test_file}")
        print("ğŸ’¡ CrÃ©e d'abord des fichiers de test dans sandbox/test_samples/")
        return
    
    # ExÃ©cute le workflow
    results = run_complete_workflow(test_file, max_iterations=3)
    
    # Sauvegarde les rÃ©sultats
    output_file = "example_workflow_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ RÃ©sultats sauvegardÃ©s dans : {output_file}")


# ============================================================
# GUIDE D'UTILISATION POUR L'ORCHESTRATEUR
# ============================================================

def print_integration_guide():
    """
    Affiche le guide d'intÃ©gration pour l'Orchestrateur.
    """
    
    guide = """
    
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                  GUIDE D'INTÃ‰GRATION POUR L'ORCHESTRATEUR                  â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    ğŸ“š IMPORTS NÃ‰CESSAIRES
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    from src.prompts import get_auditor_prompt, get_fixer_prompt, get_judge_prompt
    import google.generativeai as genai
    import json
    
    
    ğŸ”§ CONFIGURATION
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
    model = genai.GenerativeModel('gemini-2.5-flash')
    
    
    ğŸ“‹ WORKFLOW DE BASE (3 Ã©tapes)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    1ï¸âƒ£ AUDITEUR (DÃ©tection des bugs)
    
       prompt = get_auditor_prompt(filename, code_content)
       response = model.generate_content(prompt)
       audit_report = json.loads(response.text)
       bugs_found = audit_report["total_issues"]
    
    
    2ï¸âƒ£ CORRECTEUR (Correction des bugs)
    
       prompt = get_fixer_prompt(filename, code_content, audit_report)
       response = model.generate_content(prompt)
       fixed_code = response.text
    
    
    3ï¸âƒ£ TESTEUR (Validation)
    
       # ExÃ©cute pytest
       import subprocess
       result = subprocess.run(['pytest', test_file], capture_output=True)
       
       prompt = get_judge_prompt(filename, result.stdout.decode())
       response = model.generate_content(prompt)
       judge_report = json.loads(response.text)
       decision = judge_report["decision"]  # "VALIDATE" ou "PASS_TO_FIXER"
    
    
    ğŸ”„ BOUCLE DE FEEDBACK
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    max_iterations = 10
    iteration = 0
    
    while iteration < max_iterations:
        # 1. Audit
        audit_report = audit(code)
        if audit_report["total_issues"] == 0:
            break  # Code propre
        
        # 2. Fix
        code = fix(code, audit_report)
        
        # 3. Test
        judge_report = test(code)
        if judge_report["decision"] == "VALIDATE":
            break  # SuccÃ¨s !
        
        iteration += 1
    
    
    ğŸ›¡ï¸ GESTION D'ERREURS
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    # Nettoyer les balises markdown dans le JSON
    response_text = response.text.strip()
    if response_text.startswith("```json"):
        response_text = response_text[7:-3]
    
    try:
        data = json.loads(response_text)
    except json.JSONDecodeError as e:
        print(f"Erreur JSON : {e}")
        # Logger et gÃ©rer l'erreur
    
    
    ğŸ“Š LOGGING (Pour le Data Officer)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    from src.utils.logger import log_experiment, ActionType
    
    log_experiment(
        agent_name="Auditor",
        model_used="gemini-2.5-flash",
        action=ActionType.ANALYSIS,
        details={
            "file": filename,
            "input_prompt": prompt,
            "output_response": response.text,
            "bugs_found": audit_report["total_issues"]
        },
        status="SUCCESS"
    )
    
    
    ğŸ’° ESTIMATIONS DE COÃ›TS
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    from src.prompts import ESTIMATED_COSTS
    
    # Workflow complet : ~9050 tokens (~12 secondes)
    total_cost = ESTIMATED_COSTS["total_workflow"]["total_tokens_avg"]
    
    
    âœ… POINTS D'ATTENTION
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    âš ï¸  Toujours nettoyer les rÃ©ponses JSON (enlever ```json si prÃ©sent)
    âš ï¸  Limiter les itÃ©rations (max 10) pour Ã©viter les boucles infinies
    âš ï¸  Logger chaque interaction avec les agents
    âš ï¸  VÃ©rifier la syntaxe du code corrigÃ© avec compile()
    âš ï¸  GÃ©rer les cas oÃ¹ pytest plante
    
    
    
    """
    
    print(guide)


if __name__ == "__main__":
    # Affiche le guide
    print_integration_guide()
    
    # Lance l'exemple
    print("\n\n" + "ğŸš€" * 40)
    print("LANCEMENT DE L'EXEMPLE")
    print("ğŸš€" * 40 + "\n")
    
    example_usage()
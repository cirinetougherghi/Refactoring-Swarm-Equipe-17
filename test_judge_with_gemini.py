"""
Script de test pour l'Agent Testeur (Judge)
Test avec diff√©rentes sorties pytest simul√©es
"""

import os
import json
from dotenv import load_dotenv
import google.generativeai as genai
from src.prompts.judge_prompt import get_judge_prompt
# ‚úÖ AJOUT DATA OFFICER : Import du syst√®me de logging
from src.utils.logger import log_experiment, ActionType

# Charge les variables d'environnement
load_dotenv()

# Configure l'API Gemini
api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    print("‚ùå ERREUR : Cl√© API non trouv√©e dans .env")
    exit(1)

genai.configure(api_key=api_key)


# ============================================================================
# SORTIES PYTEST SIMUL√âES
# ============================================================================

PYTEST_ALL_PASS = """============================= test session starts ==============================
platform darwin -- Python 3.11.0, pytest-7.4.0
collected 5 items

test_code.py::test_calculate_average PASSED                           [ 20%]
test_code.py::test_calculate_average_empty PASSED                     [ 40%]
test_code.py::test_process_data PASSED                                [ 60%]
test_code.py::test_multiply_by_two PASSED                             [ 80%]
test_code.py::test_multiply_by_two_negative PASSED                    [100%]

============================== 5 passed in 0.12s ===============================
"""

PYTEST_SOME_FAIL = """============================= test session starts ==============================
platform darwin -- Python 3.11.0, pytest-7.4.0
collected 5 items

test_code.py::test_calculate_average PASSED                           [ 20%]
test_code.py::test_calculate_average_empty FAILED                     [ 40%]
test_code.py::test_process_data PASSED                                [ 60%]
test_code.py::test_multiply_by_two FAILED                             [ 80%]
test_code.py::test_multiply_by_two_negative PASSED                    [100%]

=================================== FAILURES ===================================
______________________ test_calculate_average_empty ________________________
    def test_calculate_average_empty():
        result = calculate_average([])
>       assert result == 0
E       assert None == 0

test_code.py:12: AssertionError
__________________________ test_multiply_by_two ____________________________
    def test_multiply_by_two():
        result = multiply_by_two(5)
>       assert result == 10
E       AssertionError: assert 11 == 10

test_code.py:25: AssertionError
========================= 2 failed, 3 passed in 0.15s ==========================
"""

PYTEST_EXECUTION_ERROR = """============================= test session starts ==============================
platform darwin -- Python 3.11.0, pytest-7.4.0
collected 0 items / 1 error

=================================== ERRORS =====================================
__________________ ERROR collecting test_code.py __________________
test_code.py:2: in <module>
    import math
E   ImportError: No module named 'math'
=========================== 1 error in 0.03s ====================================
"""

PYTEST_NO_TESTS = """============================= test session starts ==============================
platform darwin -- Python 3.11.0, pytest-7.4.0
collected 0 items

============================ no tests ran in 0.01s ==============================
"""

PYTEST_SYNTAX_ERROR = """============================= test session starts ==============================
platform darwin -- Python 3.11.0, pytest-7.4.0
collected 0 items / 1 error

=================================== ERRORS =====================================
__________________ ERROR collecting test_code.py __________________
test_code.py:10: in <module>
    def calculate_average(numbers)
E     File "test_code.py", line 10
E       def calculate_average(numbers)
E                                     ^
E   SyntaxError: invalid syntax
=========================== 1 error in 0.02s ====================================
"""


def test_judge_case(case_name: str, file_name: str, pytest_output: str):
    """
    Teste le Testeur sur un cas sp√©cifique.
    
    Args:
        case_name (str): Nom du cas de test
        file_name (str): Nom du fichier test√©
        pytest_output (str): Sortie pytest √† analyser
    """
    print("=" * 80)
    print(f"üß™ TEST : {case_name}")
    print("=" * 80)
    
    # G√©n√®re le prompt
    print("‚öôÔ∏è  G√©n√©ration du prompt...")
    prompt = get_judge_prompt(file_name, pytest_output)
    print(f"‚úÖ Prompt g√©n√©r√© ({len(prompt)} caract√®res, ~{len(prompt)//4} tokens)")
    
    # Envoie √† Gemini
    print("ü§ñ Envoi √† Gemini pour analyse...")
    try:
        model = genai.GenerativeModel('gemini-2.5-flash')
        response = model.generate_content(prompt)
        raw_response = response.text

        print(f"‚úÖ R√©ponse re√ßue ({len(raw_response)} caract√®res)")
         # ‚úÖ AJOUT DATA OFFICER : Log de l'interaction r√©ussie
        log_experiment(
            agent_name="Judge_Agent",
            model_used="gemini-2.5-flash",
            action=ActionType.DEBUG,
            details={
                "test_case": case_name,
                "file_tested": file_name,
                "input_prompt": prompt,
                "output_response": raw_response,
                "prompt_length_chars": len(prompt),
                "response_length_chars": len(raw_response),
                "pytest_output_lines": len(pytest_output.splitlines())
            },
            status="SUCCESS"
        )
        
    except Exception as e:
        print(f"‚ùå Erreur lors de l'appel API : {e}")
         # ‚úÖ AJOUT DATA OFFICER : Log de l'erreur API
        log_experiment(
            agent_name="Judge_Agent",
            model_used="gemini-2.5-flash",
            action=ActionType.DEBUG,
            details={
                "test_case": case_name,
                "file_tested": file_name,
                "input_prompt": prompt,
                "output_response": "",
                "error_type": type(e).__name__,
                "error_message": str(e)
            },
            status="ERROR"
        )
        return
    
    # Affiche la r√©ponse brute
    print("\n" + "=" * 80)
    print("üì® R√âPONSE BRUTE DE GEMINI :")
    print("=" * 80)
    print(raw_response)
    print("=" * 80)
    
    # Parse le JSON
    try:
        # Nettoie la r√©ponse
        cleaned = raw_response.strip()
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        if cleaned.startswith("```"):
            cleaned = cleaned[3:]
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
        cleaned = cleaned.strip()
        
        result = json.loads(cleaned)
        print("\n‚úÖ JSON VALIDE !")
          # ‚úÖ AJOUT DATA OFFICER : Log enrichi avec r√©sultats du parsing
        log_experiment(
            agent_name="Judge_Agent",
            model_used="gemini-2.5-flash",
            action=ActionType.DEBUG,
            details={
                "test_case": case_name,
                "file_tested": file_name,
                "input_prompt": prompt,
                "output_response": raw_response,
                "parsing_status": "SUCCESS",
                "json_valid": True,
                "decision": result.get('decision', 'N/A'),
                "total_tests": result.get('total_tests', 0),
                "passed_tests": result.get('passed', 0),
                "failed_tests": result.get('failed', 0),
                "errors_count": len(result.get('errors', []))
            },
            status="SUCCESS"
        )
        
        # Affiche les r√©sultats
        print(f"\nüìä R√âSULTAT DE L'ANALYSE :")
        print(f"   Fichier : {result.get('file', 'N/A')}")
        print(f"   D√©cision : {result.get('decision', 'N/A')}")
        print(f"   Tests totaux : {result.get('total_tests', 0)}")
        print(f"   R√©ussis : {result.get('passed', 0)}")
        print(f"   √âchou√©s : {result.get('failed', 0)}")
        print(f"   Message : {result.get('message', 'N/A')}")
        
        # Affiche les erreurs
        errors = result.get('errors', [])
        if errors:
            print(f"\n‚ùå ERREURS D√âTECT√âES ({len(errors)}) :")
            for i, error in enumerate(errors, 1):
                print(f"   [{i}] Test : {error.get('test_name', 'N/A')}")
                print(f"       Type : {error.get('error_type', 'N/A')}")
                print(f"       Message : {error.get('message', 'N/A')}")
                print(f"       Location : {error.get('location', 'N/A')}")
        
        # Sauvegarde le r√©sultat
        output_file = f"results_judge_{case_name.lower().replace(' ', '_')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        print(f"\nüíæ R√©sultat sauvegard√© dans : {output_file}")
        
    except json.JSONDecodeError as e:
        print(f"\n‚ùå ERREUR : JSON INVALIDE !")
        print(f"   Erreur : {e}")
                # ‚úÖ AJOUT DATA OFFICER : Log de l'√©chec du parsing
        log_experiment(
            agent_name="Judge_Agent",
            model_used="gemini-2.5-flash",
            action=ActionType.DEBUG,
            details={
                "test_case": case_name,
                "file_tested": file_name,
                "input_prompt": prompt,
                "output_response": raw_response,
                "parsing_status": "FAILED",
                "json_valid": False,
                "parsing_error_type": type(e).__name__,
                "parsing_error_message": str(e)
            },
            status="PARTIAL"
        )

        
        # Sauvegarde la r√©ponse brute
        error_file = f"debug_judge_{case_name.lower().replace(' ', '_')}.txt"
        with open(error_file, 'w', encoding='utf-8') as f:
            f.write(raw_response)
        print(f"\nüíæ R√©ponse brute sauvegard√©e dans : {error_file}")
    
    print("\n" + "=" * 80)


def main():
    """Fonction principale - teste tous les cas"""
    
    print("\n" + "üß™" * 40)
    print("TEST DE L'AGENT TESTEUR AVEC GEMINI 2.5 FLASH")
    print("üß™" * 40 + "\n")
    
    # Liste des cas √† tester
    test_cases = [
        ("Cas 1 - Tous les tests passent", "buggy_code_simple.py", PYTEST_ALL_PASS),
        ("Cas 2 - Quelques tests √©chouent", "buggy_code_simple.py", PYTEST_SOME_FAIL),
        ("Cas 3 - Erreur d'ex√©cution", "buggy_code_simple.py", PYTEST_EXECUTION_ERROR),
        ("Cas 4 - Aucun test", "buggy_code_simple.py", PYTEST_NO_TESTS),
        ("Cas 5 - Erreur de syntaxe", "buggy_code_simple.py", PYTEST_SYNTAX_ERROR),
    ]
    
    for case_name, file_name, pytest_output in test_cases:
        test_judge_case(case_name, file_name, pytest_output)
        print("\n")
    
    print("‚úÖ TOUS LES TESTS TERMIN√âS !\n")
    print("\nüìä Les logs d'exp√©rimentation ont √©t√© enregistr√©s dans logs/experiment_data.json")
    print("üí° Lancez 'python validate_logs.py' pour valider le format des logs\n")
    
    # R√©sum√©
    print("=" * 80)
    print("üìä R√âSUM√â DES CAS TEST√âS")
    print("=" * 80)
    print("‚úÖ Cas 1 : Tous passent ‚Üí Devrait retourner VALIDATE")
    print("‚úÖ Cas 2 : Quelques √©chouent ‚Üí Devrait retourner PASS_TO_FIXER")
    print("‚úÖ Cas 3 : Erreur ex√©cution ‚Üí Devrait retourner PASS_TO_FIXER")
    print("‚úÖ Cas 4 : Aucun test ‚Üí Devrait retourner PASS_TO_FIXER")
    print("‚úÖ Cas 5 : Erreur syntaxe ‚Üí Devrait retourner PASS_TO_FIXER")
    print("=" * 80)


if __name__ == "__main__":
    main()